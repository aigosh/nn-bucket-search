{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_treshold(file='../data/treshold.npy'):\n",
    "    return np.load(file)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = load_treshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_string_array(arr, dtype=None):\n",
    "    return np.fromstring(arr[1:-1], dtype=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_string_ndarray(arr, dtype=None):\n",
    "    return np.array([read_string_array(target, dtype=None) for target in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.csv'\n",
    "validation_file = '../data/validation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(chunk):\n",
    "    X = chunk['image'].values\n",
    "    y = chunk['label'].to_numpy(dtype=np.int)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(X, y, device=torch.device('cpu')):\n",
    "    return torch.tensor(X, device=device), torch.tensor(y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_iterator(df, chunksize=1, limit=None):\n",
    "    length = len(df)\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        mn = i * chunksize\n",
    "        mx = mn + min(chunksize, length)\n",
    "\n",
    "        if limit is not None:\n",
    "            mx = min(mx, limit)\n",
    "        \n",
    "        yield df[mn:mx]\n",
    "        i += 1\n",
    "        \n",
    "        if mx >= length or (limit is not None and mx >= limit):\n",
    "            raise StopIteration\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_iterator(df, chunksize=50000):\n",
    "    unique_labels = df[\"label\"].unique()\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        positive = df[df[\"label\"] == label]\n",
    "        negative = df[df[\"label\"] != label]\n",
    "\n",
    "        size = int(chunksize / 4)\n",
    "        positive_1 = positive.sample(size, replace=True)\n",
    "        positive_2 = positive.sample(size, replace=True)\n",
    "        positive_3 = positive.sample(size, replace=True)\n",
    "        negative_1 = negative.sample(size, replace=True)\n",
    "        \n",
    "        x1_1, y1_1 = read_chunk(positive_1)\n",
    "        x1_1 = np.array(x1_1.tolist(), dtype=np.float)\n",
    "        \n",
    "        x1_2, y1_2 = read_chunk(positive_2)\n",
    "        x1_2 = np.array(x1_2.tolist(), dtype=np.float)\n",
    "        \n",
    "        x2_1, y2_1 = read_chunk(positive_3)\n",
    "        x2_1 = np.array(x2_1.tolist(), dtype=np.float)\n",
    "        \n",
    "        x2_2, y2_2 = read_chunk(negative_1)\n",
    "        x2_2 = np.array(x2_2.tolist(), dtype=np.float)\n",
    "        \n",
    "        x1 = np.array(list(zip(x1_1, x1_2)))\n",
    "        x1 = np.concatenate(x1, axis=0)\n",
    "        y1 = (y1_1 == y1_2).reshape(-1, 1).astype(np.int)\n",
    "        \n",
    "        x2 = np.array(list(zip(x2_1, x2_2)))\n",
    "        x2 = np.concatenate(x2, axis=0)\n",
    "        y2 = -(y2_1 != y2_2).reshape(-1, 1).astype(np.int)\n",
    "        \n",
    "        X = np.concatenate([x1, x2])\n",
    "        Y = np.concatenate([y1, y2])\n",
    "        \n",
    "        yield X, Y\n",
    "    raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triples_iterator(df, size=10000):\n",
    "    df = shuffle(df)\n",
    "    unique_labels = df[\"label\"].unique()\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        positive = df[df[\"label\"] == label]\n",
    "        negative = df[df[\"label\"] != label]\n",
    "\n",
    "        anchor = positive.sample(size, replace=True)\n",
    "        positives = positive.sample(size, replace=True)\n",
    "        negatives = negative.sample(size, replace=True)\n",
    "        \n",
    "        x1, _ = read_chunk(anchor)\n",
    "        x1 = np.array(x1.tolist(), dtype=np.float)\n",
    "        \n",
    "        x2, _ = read_chunk(positives)\n",
    "        x2 = np.array(x2.tolist(), dtype=np.float)\n",
    "        \n",
    "        x3, y_ = read_chunk(negatives)\n",
    "        x3 = np.array(x3.tolist(), dtype=np.float)\n",
    "        \n",
    "        X = np.array(list(zip(x1, x2, x3)))\n",
    "        X = np.concatenate(X, axis=0)\n",
    "        \n",
    "        yield X\n",
    "    raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(x1, x2, y, k=5):\n",
    "    y = y > 0\n",
    "    is_same = y.reshape(len(y))\n",
    "    y = y[is_same]\n",
    "    count = torch.sum(is_same)\n",
    "    \n",
    "    if not count:\n",
    "        return 1.0\n",
    "    \n",
    "    x1 = x1.detach()[is_same, :].argmax(axis=1).reshape(-1, 1)\n",
    "    x2 = -x2.detach()[is_same, :]\n",
    "    x2 = x2.argsort(axis=1)[:, :k]\n",
    "    y_hat = x1 == x2\n",
    "    y_hat = torch.sum(y_hat, dim=1).reshape(-1, 1) > 0\n",
    "    y = y > 0\n",
    "    recall = torch.sum(y == y_hat).type(torch.float) / count\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_recall_at_k(anchor, positives, k=5):\n",
    "    anchor = anchor.detach().argmax(axis=1).view(-1, 1)\n",
    "    positives = -positives.detach()\n",
    "    positives = positives.argsort(axis=1)[:, :k]\n",
    "    y_hat = anchor == positives\n",
    "    y_hat = torch.sum(y_hat, dim=1).view(-1, 1) > 0\n",
    "    recall = torch.sum(y_hat).type(torch.float) / len(anchor)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_at_k(torch.tensor([[0.9, 0.1, 0.5, 0.6, 0.7],[0.1, 0.5 , 0.2, 0.3, 0.1]]), \n",
    "            torch.tensor([[1.0, 0.0, 0.2, 0.3, 0.4], [0.2, 1 , 0.3, 0.5, 0.6]]),\n",
    "            torch.tensor([[1],[-1]]), k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_file,\n",
    "                 header=0,\n",
    "                 converters={'image': read_string_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.102006525, -0.202537119, 0.113066137, 0.14...</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.118899353, 0.101335108, 0.0376838185, 0.48...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.260352045, -0.338344872, -0.137531623, -0....</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0528041609, -0.177006856, 0.0429622903, -0....</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.212558284, 0.015732646, 0.230285764, 0.069...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  label\n",
       "0  [-0.102006525, -0.202537119, 0.113066137, 0.14...     47\n",
       "1  [-0.118899353, 0.101335108, 0.0376838185, 0.48...     18\n",
       "2  [-0.260352045, -0.338344872, -0.137531623, -0....     41\n",
       "3  [0.0528041609, -0.177006856, 0.0429622903, -0....     12\n",
       "4  [-0.212558284, 0.015732646, 0.230285764, 0.069...     29"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, tau=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "\n",
    "        self.embeddings = Parameter(torch.empty((128, 512)).uniform_(-0.5, 0.5).requires_grad_(True))\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        \n",
    "    def update_temperature(self, tau):\n",
    "        self.tau = tau\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "#         x = F.tanh(x)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        z = self.bn2(z)\n",
    "        z = F.tanh(z)\n",
    "        z = self.fc4(z)\n",
    "#         z = F.tanh(z)\n",
    "        return z\n",
    "    \n",
    "    def reparametrize(self, q_y):\n",
    "#         print(q_y)\n",
    "#         print(self.embeddings)\n",
    "        z = F.gumbel_softmax(q_y, tau=self.tau, dim=-1)\n",
    "        z = torch.mm(z, self.embeddings)\n",
    "#         print(z)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoding = self.encode(x)\n",
    "        z = self.reparametrize(encoding)\n",
    "        return self.decode(z), z, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, tau=1):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.tau = tau\n",
    "        \n",
    "#         self.fc1 = nn.Linear(512, 256)\n",
    "#         self.bn1 = nn.BatchNorm1d(256)\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.bn2 = nn.BatchNorm1d(128)\n",
    "#         self.fc3 = nn.Linear(128, 128 * 512)\n",
    "\n",
    "# #         self.embeddings = Parameter(torch.empty((128, 512)).uniform_(-0.5, 0.5).requires_grad_(True))\n",
    "        \n",
    "#         self.fc4 = nn.Linear(128 * 512, 128)\n",
    "#         self.bn3 = nn.BatchNorm1d(128)\n",
    "#         self.fc5 = nn.Linear(128, 256)\n",
    "#         self.bn4 = nn.BatchNorm1d(256)\n",
    "#         self.fc6 = nn.Linear(256, 512)\n",
    "        \n",
    "#     def update_temperature(self, tau):\n",
    "#         self.tau = tau\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = F.tanh(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = F.tanh(x)\n",
    "# #         x = F.tanh(x)\n",
    "#         return x\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         z = z.view(z.size(0), 128 * 512)\n",
    "#         z = self.fc4(z)\n",
    "#         z = self.bn3(z)\n",
    "#         z = F.tanh(z)\n",
    "#         z = self.fc5(z)\n",
    "#         z = self.bn4(z)\n",
    "#         z = F.tanh(z)\n",
    "#         z = self.fc6(z)\n",
    "# #         z = F.tanh(z)\n",
    "#         return z\n",
    "    \n",
    "#     def reparametrize(self, q_y):\n",
    "# #         print(q_y)\n",
    "# #         print(self.embeddings)\n",
    "#         z = F.gumbel_softmax(q_y, tau=self.tau, dim=-1)\n",
    "#         z = self.fc3(z)\n",
    "#         z = z.view(z.size(0), 128, 512)\n",
    "        \n",
    "# #         z = torch.mm(z, self.embeddings)\n",
    "# #         print(z)\n",
    "#         return z\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         encoding = self.encode(x)\n",
    "#         z = self.reparametrize(encoding)\n",
    "#         return self.decode(z), z, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss, L1Loss, CosineEmbeddingLoss, KLDivLoss, BCELoss, TripletMarginLoss, BCEWithLogitsLoss\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', margin=0):\n",
    "        super(CosineLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.reduction = reduction\n",
    "    def forward(self, x1, x2, y):\n",
    "        sim = F.cosine_similarity(x1, x2, dim=1).view(-1, 1)\n",
    "        y = y.view(-1)\n",
    "        positive = y > 0\n",
    "        positive_loss = 1 - positive * sim\n",
    "        negative = y < 0\n",
    "        negative_loss = negative * sim - self.margin\n",
    "        negative_loss = torch.max(\n",
    "            torch.zeros(negative_loss.shape, device=device),\n",
    "            negative_loss\n",
    "        )\n",
    "\n",
    "        if self.reduction == 'sum':\n",
    "            return torch.sum(positive_loss) + torch.sum(negative_loss)\n",
    "\n",
    "        return torch.mean(positive_loss + negative_loss)\n",
    "\n",
    "class TripletCosineLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', margin=0):\n",
    "        super(TripletCosineLoss, self).__init__()\n",
    "        self.margin = torch.tensor(margin, dtype=torch.float, device=device)\n",
    "        self.reduction = reduction\n",
    "    def forward(self, anchor, positives, negatives):\n",
    "        sim_pos = F.cosine_similarity(anchor, positives, dim=1).view(-1, 1)\n",
    "        sim_neg = F.cosine_similarity(anchor, negatives, dim=1).view(-1, 1)\n",
    "        reductor = torch.mean\n",
    "        \n",
    "        if self.reduction == 'sum':\n",
    "            reductor = torch.sum\n",
    "\n",
    "#         zero = torch.zeros(dist1.shape, device=device)\n",
    "#         loss = torch.max(zero, self.margin - (dist1 + dist2))\n",
    "        loss = F.relu(self.margin - sim_pos + sim_neg)\n",
    "        return reductor(loss)        \n",
    "        \n",
    "class VaeLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', margin=0):\n",
    "        super(VaeLoss, self).__init__()\n",
    "        self.cosine_loss = CosineLoss(margin=margin, reduction=reduction)\n",
    "        self.mse_loss = MSELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, real, pred, x1, x2, y):\n",
    "        cos_loss = self.cosine_loss(x1, x2, y)\n",
    "#         print(cos_loss)\n",
    "        mse_loss = self.mse_loss(real, pred)\n",
    "\n",
    "        return cos_loss + mse_loss, cos_loss, mse_loss\n",
    "\n",
    "# class VaeTripletLoss(nn.Module):\n",
    "#     def __init__(self, reduction='mean', margin=0):\n",
    "#         super(VaeTripletLoss, self).__init__()\n",
    "# #         self.cos_loss = CosineLoss(margin=margin, reduction=reduction)\n",
    "#         self.trilpet_loss = TripletCosineLoss(margin=margin, reduction=reduction)\n",
    "#         self.mse_loss = MSELoss(reduction=reduction)\n",
    "\n",
    "#     def forward(self, real, pred, anchor, positive, negative):\n",
    "#         trilpet_loss = self.trilpet_loss(anchor, positive, negative)\n",
    "# #         print(cos_loss)\n",
    "# #         y_pos = torch.tensor([1] * len(anchor), device=device).view(-1, 1)\n",
    "# #         y_neg = -y_pos\n",
    "#         mse_loss = self.mse_loss(real, pred)\n",
    "# #         cos_loss_1 = self.cos_loss(anchor, positive, y_pos)\n",
    "# #         cos_loss_2 = self.cos_loss(anchor, negative, y_neg)\n",
    "# #         cos_loss = (cos_loss_1 + cos_loss_2) / 2\n",
    "\n",
    "#         return trilpet_loss + mse_loss * 10, trilpet_loss, mse_loss, 0\n",
    "class VaeTripletLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', margin=0):\n",
    "        super(VaeTripletLoss, self).__init__()\n",
    "#         self.cos_loss = CosineLoss(margin=margin, reduction=reduction)\n",
    "        self.trilpet_loss = TripletCosineLoss(margin=margin, reduction=reduction)\n",
    "        self.mse_loss = MSELoss(reduction=reduction)\n",
    "\n",
    "    def forward(self, real, pred, anchor, positive, negative):\n",
    "        trilpet_loss = self.trilpet_loss(anchor, positive, negative)\n",
    "#         print(cos_loss)\n",
    "#         y_pos = torch.tensor([1] * len(anchor), device=device).view(-1, 1)\n",
    "#         y_neg = -y_pos\n",
    "        mse_loss = self.mse_loss(real, pred)\n",
    "#         cos_loss_1 = self.cos_loss(anchor, positive, y_pos)\n",
    "#         cos_loss_2 = self.cos_loss(anchor, negative, y_neg)\n",
    "#         cos_loss = (cos_loss_1 + cos_loss_2) / 2\n",
    "\n",
    "        return trilpet_loss + mse_loss * 10, trilpet_loss, mse_loss, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.load_state_dict(torch.load('vae.torch', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_loss = VaeLoss(margin=0, reduction='mean')\n",
    "vae_triplet_loss = VaeTripletLoss(margin=1, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(lr, models):\n",
    "#     return torch.optim.SGD([{'params': model.parameters()} for model in models],\n",
    "#                     lr=lr, nesterov=True, momentum=0.9)\n",
    "    return torch.optim.Adam([{'params': model.parameters()} for model in models],\n",
    "                    lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.train(True)\n",
    "# lr = 0.99\n",
    "# for epoch in range(10):\n",
    "#     train_loss = 0.\n",
    "#     train_cos_loss = 0.\n",
    "#     train_mse_loss = 0.\n",
    "#     train_recall = 0.\n",
    "#     count = 0\n",
    "#     optimizer = get_optimizer(lr, [vae])\n",
    "#     for X, y in pairs_iterator(df_train, chunksize=40000):\n",
    "#         X = torch.tensor(X, device=device, dtype=torch.float, requires_grad=True)\n",
    "#         y = torch.tensor(y, device=device, dtype=torch.float)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         decoded, thetas, encoding = vae(X)\n",
    "        \n",
    "#         pred_1, pred_2 = encoding[0::2], encoding[1::2]\n",
    "#         x1, x2 = thetas[0::2], thetas[1::2]\n",
    "#         recall = recall_at_k(pred_1, pred_2, y)\n",
    "#         loss, cos_loss, mse_loss = vae_loss(X, decoded, x1, x2, y, -1)\n",
    "# #         loss = criterion(x1, x2, y)\n",
    "#         loss.backward()\n",
    "# #         print(head.embeddings.grad)\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "#         train_cos_loss += cos_loss.item()\n",
    "#         train_mse_loss += mse_loss.item()\n",
    "#         train_recall += recall.cpu().detach().numpy()\n",
    "#         count += 1\n",
    "#         print('Epoch:', epoch, \n",
    "#               'Loss:', train_loss / count, \n",
    "#               'Recall:', train_recall / count, \n",
    "#               'Cos Loss:', train_cos_loss / count, \n",
    "#               'MSE Loss:', train_mse_loss / count)\n",
    "        \n",
    "#         if count % 100:\n",
    "#             torch.save(vae.state_dict(), 'vae.torch')\n",
    "#     lr = max(lr / 10, 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a.y.igoshin/virtual_environments/main/lib/python3.6/site-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/a.y.igoshin/virtual_environments/main/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: generator 'triples_iterator' raised StopIteration\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "vae.train(True)\n",
    "lr = 1e-3\n",
    "tau_0 = 10.0\n",
    "for epoch in range(20):\n",
    "    train_loss = 0.\n",
    "    train_cos_loss = 0.\n",
    "    train_mse_loss = 0.\n",
    "    train_triplet_loss = 0.\n",
    "    train_recall = 0.\n",
    "    count = 0\n",
    "    vae.update_temperature(tau_0)\n",
    "    optimizer = get_optimizer(lr, [vae])\n",
    "#     for X in triples_iterator(df_train, size=10000):\n",
    "    for X in triples_iterator(df_train, size=1000):\n",
    "        X = torch.tensor(X, device=device, dtype=torch.float, requires_grad=True)\n",
    "#         y = torch.tensor(y, device=device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoded, thetas, encoding = vae(X)\n",
    "        \n",
    "        pred_anchor, pred_positive = encoding[0::3], encoding[1::3]\n",
    "        anchor, positive, negative = thetas[0::3], thetas[1::3], thetas[2::3]\n",
    "        loss, triplet_loss, mse_loss, cos_loss = vae_triplet_loss(X, decoded, anchor, positive, negative)\n",
    "#         loss = criterion(x1, x2, y)\n",
    "        loss.backward()\n",
    "#         print(vae.embeddings.grad)\n",
    "#         print(vae.embeddings)\n",
    "        optimizer.step()\n",
    "#         print(vae.embeddings)\n",
    "#         for param in vae.parameters():\n",
    "#             print(param)\n",
    "        recall = triplet_recall_at_k(pred_anchor, pred_positive)\n",
    "        train_loss += loss.item()\n",
    "#         train_cos_loss += cos_loss.item()\n",
    "        train_mse_loss += mse_loss.item()\n",
    "        train_triplet_loss += triplet_loss.item()\n",
    "        train_recall += recall.detach().cpu().numpy()\n",
    "        count += 1\n",
    "        writer.add_scalar('Epoch {}/Loss/VAE'.format(epoch), train_loss / count, count)\n",
    "        writer.add_scalar('Epoch {}/Recall'.format(epoch), train_recall / count, count)\n",
    "        writer.add_scalar('Epoch {}/Loss/Triplet'.format(epoch), train_triplet_loss / count, count)\n",
    "        writer.add_scalar('Epoch {}/Loss/MSE'.format(epoch), train_mse_loss / count, count)\n",
    "        writer.add_scalar('Epoch {}/Temperature'.format(epoch), vae.tau, count)\n",
    "#         print('Epoch:', epoch, \n",
    "#               'Loss:', train_loss / count, \n",
    "#               'Recall:', train_recall / count, \n",
    "#               'Triplet Loss:', train_triplet_loss / count, \n",
    "# #               'Cos Loss:', train_cos_loss / count, \n",
    "#               'MSE Loss:', train_mse_loss / count)\n",
    "        \n",
    "        if count % 400 == 0 and count > 0:\n",
    "#             pass\n",
    "            vae.update_temperature(np.maximum(vae.tau * np.exp(-0.00003 * count), 0.5))\n",
    "#             print(vae.tau)\n",
    "        if count % 100 == 0:\n",
    "            torch.save(vae.state_dict(), 'vae1.torch')\n",
    "    lr = max(lr / 10, 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae1.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cluster_with_vae(vae, file, chunksize=100000):\n",
    "    vae.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        df = pd.read_csv(file,\n",
    "                         chunksize=chunksize,\n",
    "                         header=0,\n",
    "                         converters={'image': read_string_array})\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for chunk in df:\n",
    "            print(count + len(chunk))\n",
    "            X, _ = read_chunk(chunk)\n",
    "            X = np.array(X.tolist(), dtype=np.float)\n",
    "            X = torch.tensor(X, device=device, dtype=torch.float)\n",
    "\n",
    "            clusters = vae.encode(X).detach().argmax(axis=1)\n",
    "            result.append(clusters)\n",
    "\n",
    "            count += len(chunk)\n",
    "        return torch.cat(result, 0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "343350\n"
     ]
    }
   ],
   "source": [
    "preds = predict_cluster_with_vae(vae, train_file, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buckets(cluster_predictions):\n",
    "    buckets = dict()\n",
    "    \n",
    "    for i, cluster in enumerate(cluster_predictions):\n",
    "        if buckets.get(cluster, None) is None:\n",
    "            buckets.update({cluster: []})\n",
    "        buckets[cluster].append(i)\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = get_buckets(cluster_predictions=preds)\n",
    "# clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126032\n",
      "3188\n",
      "3508\n",
      "1451\n",
      "2850\n",
      "5074\n",
      "1792\n",
      "1368\n",
      "702\n",
      "751\n",
      "6333\n",
      "2100\n",
      "3695\n",
      "4829\n",
      "4262\n",
      "1343\n",
      "1004\n",
      "1884\n",
      "7307\n",
      "6261\n",
      "625\n",
      "2418\n",
      "8038\n",
      "6156\n",
      "665\n",
      "1980\n",
      "620\n",
      "1611\n",
      "2960\n",
      "1909\n",
      "6235\n",
      "779\n",
      "1919\n",
      "2796\n",
      "1650\n",
      "337\n",
      "850\n",
      "6604\n",
      "578\n",
      "11094\n",
      "2678\n",
      "826\n",
      "3873\n",
      "1577\n",
      "7796\n",
      "1461\n",
      "2791\n",
      "4800\n",
      "1107\n",
      "1931\n",
      "928\n",
      "1506\n",
      "4304\n",
      "1459\n",
      "1167\n",
      "2142\n",
      "966\n",
      "468\n",
      "1433\n",
      "993\n",
      "642\n",
      "1287\n",
      "1297\n",
      "1050\n",
      "959\n",
      "702\n",
      "1058\n",
      "13\n",
      "587\n",
      "2532\n",
      "397\n",
      "474\n",
      "1119\n",
      "1494\n",
      "1743\n",
      "769\n",
      "1341\n",
      "853\n",
      "1279\n",
      "675\n",
      "863\n",
      "451\n",
      "1040\n",
      "1585\n",
      "2822\n",
      "1137\n",
      "760\n",
      "2458\n",
      "1067\n",
      "712\n",
      "1786\n",
      "756\n",
      "1871\n",
      "1207\n",
      "1596\n",
      "648\n",
      "1370\n",
      "792\n",
      "902\n",
      "341\n",
      "1626\n",
      "643\n",
      "3154\n",
      "669\n",
      "796\n",
      "352\n",
      "197\n",
      "490\n",
      "659\n",
      "340\n",
      "99\n",
      "3\n",
      "117\n",
      "2\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in clusters:\n",
    "    print(len(clusters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_buckets(buckets, file='../data/buckets_vae.npy'):\n",
    "    np.save(file, [buckets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_buckets(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_by_indexes(df, indexes, chunksize=100000):\n",
    "    db = chunk_iterator(df, chunksize=chunksize)\n",
    "    X_result = []\n",
    "    y_result = []\n",
    "    \n",
    "    for chunk in db:\n",
    "        X, y = read_chunk(chunk)\n",
    "        X = np.take(X, indexes, mode='clip')\n",
    "        X = read_string_ndarray(X)\n",
    "        X = np.array(X.tolist(), dtype=np.float)\n",
    "        y = np.take(y, indexes, mode='clip')\n",
    "        \n",
    "        X_result.append(X)\n",
    "        y_result.append(y)\n",
    "    return np.concatenate(X_result), np.concatenate(y_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_with_vae(vae, clusters, query, df, treshold, chunksize=100000, device=device):\n",
    "    vae.eval()\n",
    "\n",
    "    similiarity = torch.nn.CosineSimilarity(dim=1)\n",
    "    similiarity = similiarity.to(device)\n",
    "    similiarity.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "\n",
    "        Q = torch.tensor([query], device=device, dtype=torch.float)\n",
    "        treshold = torch.tensor(treshold, device=device)\n",
    "        clusters_hat = vae.encode(Q)\n",
    "        clusters_sorted = torch.argsort(-clusters_hat).cpu().numpy()[0]\n",
    "        print(clusters_sorted)\n",
    "\n",
    "        for cluster_idx in clusters_sorted:\n",
    "            print(cluster_idx)\n",
    "            cluster = clusters.get(cluster_idx)\n",
    "            if not cluster:\n",
    "                continue\n",
    "            cluster = np.array(clusters[cluster_idx])\n",
    "\n",
    "            for indexes in chunk_iterator(cluster, chunksize=chunksize):\n",
    "                X, y = collect_by_indexes(df, indexes, chunksize)\n",
    "\n",
    "                X, y = to_tensor(X, y, device)\n",
    "\n",
    "                sim = similiarity(Q, X)\n",
    "\n",
    "                idx = torch.argmax(sim)\n",
    "\n",
    "                if sim[idx] > treshold:\n",
    "                    return y[idx]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(train_file, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(validation_file,\n",
    "                 chunksize=1,\n",
    "                 skiprows=50000,\n",
    "                 converters={'image': read_string_array}, names=['image', 'label'])\n",
    "imgs, label = read_chunk(df.get_chunk())\n",
    "img = np.array(imgs.tolist()[0], dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[ 94  30  71  84 125  26  63  90  39  67   4  10  93  48  24  31  49  78\n",
      "   3 122  35 120  22 110  72  76  18  36 126  44 105 103  11 116 119  19\n",
      "  61  83 121  89  29  88  97  17  98  81   6  23  12  33  57  15 124  60\n",
      "  43  50  92   7  52  66 114  32  13  79 123  53  47 111  54 118  75 115\n",
      "  14 117 127  64  99  86  38  58  34 112 104  85  80 107  96  62   9 106\n",
      "   0  16  41  65  45  73  91  37  27 102  77 108  95  40 113 101  69  56\n",
      "  87  51  20  82  28  70  46   5   2  55   8  42  68  74   1 109  25  59\n",
      "  21 100]\n",
      "94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a.y.igoshin/virtual_environments/main/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: generator 'chunk_iterator' raised StopIteration\n",
      "  \n",
      "/home/a.y.igoshin/virtual_environments/main/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: generator 'chunk_iterator' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-7026e8dd12da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabel_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_with_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-269-c63870e24e1c>\u001b[0m in \u001b[0;36mfind_with_vae\u001b[0;34m(vae, clusters, query, df, treshold, chunksize, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_by_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-271-c9e1561cd97f>\u001b[0m in \u001b[0;36mcollect_by_indexes\u001b[0;34m(df, indexes, chunksize)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_string_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-4cbbfb2bb5d2>\u001b[0m in \u001b[0;36mread_string_ndarray\u001b[0;34m(arr, dtype)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_string_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread_string_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-4cbbfb2bb5d2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_string_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread_string_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-dc76949504ad>\u001b[0m in \u001b[0;36mread_string_array\u001b[0;34m(arr, dtype)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_string_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(label[0] in train_df['label'].values)\n",
    "start = time.time()\n",
    "label_hat = find_with_vae(vae, clusters, img, train_df, 0.7, chunksize=10000)\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print('Duration:', duration)\n",
    "print('Pred:', label_hat, 'Real', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 9054), started 0:23:05 ago. (Use '!kill 9054' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-27de0850ec10f6a4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-27de0850ec10f6a4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs --port=6007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.5398e-02, 1.9197e-02, 2.8779e-02, 4.8194e-03, 3.6361e-03, 2.4457e-02,\n",
      "         5.3987e-03, 1.2014e-02, 1.4412e-01, 1.7968e-02, 4.1328e-03, 2.6410e-02,\n",
      "         4.4057e-02, 4.2040e-03, 1.5978e-03, 1.0701e-02, 1.4340e-03, 9.4143e-03,\n",
      "         4.7526e-03, 2.7632e-02, 2.3507e-03, 1.1498e-02, 6.8986e-02, 1.3603e-02,\n",
      "         3.3304e-02, 5.4837e-02, 7.4567e-03, 5.5239e-02, 1.5254e-02, 2.0303e-01,\n",
      "         1.6258e-02, 3.8065e-02],\n",
      "        [1.4381e-02, 4.9327e-03, 3.4376e-02, 3.9673e-03, 7.7558e-02, 3.6949e-03,\n",
      "         1.8732e-02, 3.0171e-02, 4.1198e-03, 1.4229e-02, 4.8411e-03, 1.3468e-01,\n",
      "         3.5893e-03, 1.3703e-02, 1.7639e-02, 8.4737e-04, 6.7688e-04, 4.7508e-04,\n",
      "         1.7213e-02, 1.2203e-01, 2.5529e-03, 2.3087e-03, 5.6765e-02, 3.4419e-01,\n",
      "         2.1852e-03, 4.1514e-03, 5.6185e-03, 4.7198e-02, 9.6883e-04, 6.1324e-04,\n",
      "         1.0351e-02, 1.2454e-03],\n",
      "        [2.6342e-03, 3.0033e-03, 7.0458e-03, 1.9195e-02, 3.5504e-03, 2.0011e-03,\n",
      "         1.4947e-03, 6.6767e-03, 1.5606e-01, 3.6947e-03, 3.1108e-02, 4.0005e-03,\n",
      "         1.9165e-03, 3.4122e-03, 6.9612e-03, 5.0870e-03, 2.5802e-01, 1.8778e-01,\n",
      "         6.9656e-03, 1.6910e-02, 2.4904e-02, 2.6986e-03, 6.2062e-03, 1.0831e-02,\n",
      "         7.7996e-02, 3.3733e-02, 8.1256e-03, 5.3154e-02, 7.1024e-04, 1.3604e-02,\n",
      "         7.7823e-03, 3.2738e-02],\n",
      "        [7.0143e-02, 1.4156e-01, 2.7016e-02, 1.7915e-02, 1.2224e-01, 4.4366e-02,\n",
      "         6.1391e-03, 6.0923e-03, 1.4383e-03, 1.9863e-03, 5.3655e-03, 1.7321e-02,\n",
      "         3.9253e-02, 4.4458e-03, 2.0095e-03, 8.8375e-03, 2.7977e-03, 1.2600e-01,\n",
      "         7.9111e-02, 7.7897e-04, 1.4840e-02, 2.6093e-02, 2.2080e-02, 5.4275e-02,\n",
      "         3.3510e-03, 1.2625e-03, 7.4866e-02, 9.7384e-04, 5.2035e-04, 3.5160e-02,\n",
      "         3.8322e-02, 3.4359e-03],\n",
      "        [5.6067e-03, 7.3034e-03, 1.0576e-03, 7.7348e-02, 6.6788e-05, 5.8748e-03,\n",
      "         7.8706e-03, 1.3430e-04, 3.1565e-03, 7.1968e-02, 6.6395e-02, 5.7539e-03,\n",
      "         1.2385e-02, 2.4436e-01, 1.6191e-04, 2.6359e-04, 7.1539e-04, 7.4541e-03,\n",
      "         1.6671e-04, 2.0583e-03, 2.6696e-04, 3.3275e-01, 1.2180e-01, 4.0911e-04,\n",
      "         1.1985e-03, 1.0541e-03, 3.7302e-03, 6.2220e-04, 4.3893e-03, 8.9076e-03,\n",
      "         4.5029e-03, 2.6431e-04],\n",
      "        [2.8752e-02, 5.3892e-03, 1.6064e-02, 2.5514e-03, 1.8195e-03, 1.2332e-02,\n",
      "         4.3092e-02, 7.2666e-02, 5.4351e-02, 7.1517e-03, 3.9912e-04, 8.8657e-02,\n",
      "         5.1173e-03, 1.5202e-03, 4.9170e-03, 1.7227e-02, 7.2447e-02, 5.8122e-03,\n",
      "         1.0661e-01, 2.9458e-01, 1.7920e-02, 3.4638e-02, 5.2104e-03, 2.0568e-02,\n",
      "         2.2654e-02, 2.0955e-02, 6.8610e-03, 9.5897e-03, 7.1208e-04, 1.4544e-02,\n",
      "         4.2711e-03, 6.1903e-04],\n",
      "        [6.2749e-04, 9.7828e-03, 2.9483e-03, 1.0736e-02, 9.6305e-02, 4.4994e-03,\n",
      "         4.8471e-02, 6.7676e-03, 4.0190e-03, 2.6421e-02, 1.4747e-01, 1.7372e-03,\n",
      "         3.4681e-03, 3.0201e-02, 5.4145e-03, 5.4719e-03, 2.2938e-02, 1.9548e-02,\n",
      "         1.7625e-03, 1.1888e-01, 1.6462e-02, 1.7360e-01, 5.6436e-03, 9.2077e-03,\n",
      "         1.5639e-02, 5.8537e-03, 8.9361e-02, 4.2548e-03, 5.8397e-03, 3.2251e-03,\n",
      "         1.8408e-03, 1.0160e-01],\n",
      "        [8.0247e-04, 3.6575e-04, 5.8428e-03, 3.2898e-02, 5.0064e-04, 6.9907e-03,\n",
      "         3.9366e-03, 2.1942e-02, 4.4235e-03, 5.6969e-03, 9.4043e-04, 5.4057e-04,\n",
      "         3.1979e-03, 1.4677e-01, 4.0566e-02, 9.2165e-03, 4.1931e-03, 3.1413e-01,\n",
      "         7.2368e-03, 1.0154e-02, 1.3636e-02, 6.2523e-02, 2.7218e-03, 4.5538e-03,\n",
      "         3.0716e-02, 3.2970e-03, 2.3813e-02, 1.9209e-01, 4.8123e-03, 2.7446e-03,\n",
      "         6.7257e-03, 3.2026e-02],\n",
      "        [4.8728e-03, 1.9826e-03, 5.1609e-02, 1.9877e-02, 1.4951e-02, 3.7003e-03,\n",
      "         9.1957e-03, 1.7269e-03, 3.2024e-03, 1.9969e-02, 1.5456e-02, 8.4831e-03,\n",
      "         5.4112e-02, 6.0004e-03, 3.3920e-01, 1.4691e-01, 1.6851e-03, 3.2671e-03,\n",
      "         9.3120e-03, 6.3343e-03, 2.5152e-03, 3.4441e-03, 2.8844e-02, 1.4988e-03,\n",
      "         2.2057e-03, 6.1786e-02, 4.6947e-02, 5.5107e-02, 6.0588e-03, 5.5609e-02,\n",
      "         6.3773e-03, 7.7613e-03],\n",
      "        [5.7000e-02, 7.4330e-03, 5.6821e-03, 5.9365e-02, 6.5229e-03, 8.0840e-02,\n",
      "         9.3671e-02, 7.0802e-02, 9.4568e-03, 1.0111e-03, 1.2209e-02, 6.4669e-03,\n",
      "         4.9263e-04, 7.5144e-03, 3.3631e-01, 3.4745e-03, 7.3799e-03, 3.4451e-03,\n",
      "         5.7207e-02, 2.1090e-02, 2.6937e-03, 1.3277e-02, 1.4754e-02, 2.1631e-02,\n",
      "         5.4497e-03, 1.0516e-02, 1.6936e-03, 3.1414e-02, 4.3648e-02, 2.7967e-03,\n",
      "         2.4554e-03, 2.3009e-03],\n",
      "        [1.0839e-03, 6.8636e-04, 2.2736e-03, 5.1189e-04, 2.9271e-03, 1.5562e-03,\n",
      "         2.9461e-03, 4.3348e-04, 9.6489e-04, 6.9370e-04, 3.4349e-03, 2.2529e-04,\n",
      "         6.7543e-04, 9.0693e-01, 2.7609e-03, 2.3671e-03, 3.8831e-04, 4.2401e-04,\n",
      "         2.3961e-04, 2.4326e-03, 1.9350e-04, 1.3590e-02, 2.1575e-03, 2.9125e-03,\n",
      "         1.6743e-05, 9.2770e-04, 3.6616e-03, 3.6060e-05, 1.2860e-03, 3.8600e-02,\n",
      "         1.9796e-04, 2.4687e-03],\n",
      "        [1.6147e-02, 5.8615e-02, 9.4586e-03, 1.1439e-01, 2.6378e-03, 1.3650e-01,\n",
      "         1.9324e-02, 2.2198e-02, 4.7876e-03, 6.2090e-03, 1.2415e-02, 3.6545e-03,\n",
      "         1.8798e-01, 3.4985e-03, 8.0241e-03, 2.8150e-02, 2.3799e-02, 2.0699e-02,\n",
      "         8.0853e-03, 3.0913e-02, 7.2826e-03, 1.2954e-02, 5.8070e-03, 6.6660e-04,\n",
      "         5.1954e-03, 1.6599e-01, 4.0172e-02, 1.7519e-03, 2.6916e-02, 1.9025e-03,\n",
      "         1.2646e-02, 1.2352e-03],\n",
      "        [7.1877e-02, 1.0248e-02, 4.9149e-04, 4.6404e-02, 4.6242e-04, 3.1366e-02,\n",
      "         1.6257e-03, 2.2600e-02, 1.4168e-01, 3.1709e-03, 2.8615e-03, 4.9446e-04,\n",
      "         6.3409e-02, 2.1128e-03, 6.8575e-04, 8.4216e-03, 1.0882e-02, 1.6611e-02,\n",
      "         1.3935e-03, 8.7633e-04, 1.4697e-01, 5.6301e-03, 2.0913e-03, 6.7756e-03,\n",
      "         1.9505e-03, 4.9781e-03, 3.6283e-03, 3.7551e-01, 2.1738e-04, 5.9211e-03,\n",
      "         8.0661e-03, 5.8675e-04],\n",
      "        [2.4877e-03, 8.3759e-03, 9.8795e-03, 1.8297e-03, 6.8308e-03, 2.7494e-02,\n",
      "         7.7699e-03, 1.0723e-01, 5.0093e-02, 8.2589e-03, 1.8630e-03, 2.0693e-01,\n",
      "         1.5289e-03, 1.3980e-02, 9.5785e-02, 9.1099e-03, 1.1393e-02, 8.9701e-03,\n",
      "         3.7781e-02, 3.9187e-02, 1.2710e-02, 3.6193e-02, 8.0687e-03, 3.2912e-03,\n",
      "         7.0056e-04, 1.5038e-03, 2.0467e-01, 1.8878e-02, 2.7169e-02, 1.0952e-03,\n",
      "         2.6275e-02, 2.6665e-03],\n",
      "        [1.1598e-01, 2.7687e-03, 1.8111e-03, 1.7773e-02, 1.0046e-03, 1.5089e-02,\n",
      "         2.1836e-02, 1.0215e-01, 4.0520e-03, 1.6895e-03, 9.3408e-04, 4.0330e-04,\n",
      "         1.1512e-04, 6.5580e-04, 2.1519e-04, 6.1987e-03, 1.0967e-03, 5.8030e-03,\n",
      "         6.0791e-03, 2.0595e-02, 2.9407e-04, 1.0694e-02, 2.1432e-04, 4.5333e-02,\n",
      "         5.3564e-01, 4.4551e-02, 1.1151e-03, 3.1704e-04, 3.3230e-03, 2.7858e-02,\n",
      "         3.9547e-03, 4.6344e-04],\n",
      "        [2.0155e-03, 4.4889e-02, 2.4350e-03, 3.2950e-03, 2.8151e-03, 9.3110e-03,\n",
      "         5.9760e-02, 2.3820e-02, 1.8207e-03, 2.1834e-03, 9.1879e-03, 2.2985e-03,\n",
      "         9.0207e-03, 1.4116e-03, 4.3473e-03, 1.6097e-02, 2.6160e-01, 6.6421e-03,\n",
      "         3.3376e-03, 3.2287e-03, 8.7418e-03, 3.5556e-03, 2.0609e-01, 5.2757e-04,\n",
      "         1.2878e-02, 9.7948e-03, 7.2533e-04, 4.9961e-03, 1.9037e-01, 1.4232e-02,\n",
      "         6.5805e-03, 7.1991e-02],\n",
      "        [1.7396e-03, 2.3629e-02, 2.5039e-02, 1.4905e-03, 1.5213e-03, 1.2520e-03,\n",
      "         1.1009e-03, 1.2689e-02, 6.7156e-03, 1.5449e-02, 3.4585e-03, 1.2086e-02,\n",
      "         1.4519e-03, 2.0857e-04, 3.7505e-03, 8.8134e-02, 2.7004e-02, 8.1599e-04,\n",
      "         1.6270e-02, 4.4147e-03, 2.7765e-04, 7.0802e-04, 4.5850e-03, 1.5116e-03,\n",
      "         1.8438e-03, 1.3632e-03, 6.3103e-03, 8.9395e-03, 6.4365e-03, 4.4007e-03,\n",
      "         3.5151e-03, 7.1189e-01],\n",
      "        [1.1155e-02, 1.4247e-02, 1.7969e-03, 2.8110e-04, 1.0984e-02, 2.2510e-03,\n",
      "         5.5237e-03, 1.4856e-02, 6.1254e-03, 4.4164e-02, 1.5041e-03, 1.4573e-02,\n",
      "         5.2898e-02, 5.1529e-02, 1.9674e-02, 1.6860e-02, 3.0580e-02, 6.3861e-03,\n",
      "         2.5057e-02, 3.3361e-01, 3.3409e-02, 2.0683e-02, 4.6033e-03, 1.5898e-03,\n",
      "         2.3811e-03, 2.1704e-01, 4.5622e-03, 8.6112e-03, 1.9042e-02, 8.1108e-03,\n",
      "         1.3017e-03, 1.4607e-02],\n",
      "        [3.5039e-01, 1.2100e-02, 1.7794e-03, 5.0368e-03, 1.4653e-01, 8.0622e-03,\n",
      "         4.1242e-02, 4.0632e-04, 1.0173e-02, 2.0026e-02, 1.0387e-02, 8.7915e-03,\n",
      "         1.8179e-04, 6.1237e-05, 1.0893e-01, 1.7820e-01, 1.5436e-03, 2.5970e-03,\n",
      "         1.5512e-02, 3.5679e-03, 3.4776e-03, 3.9353e-03, 2.4772e-03, 7.2864e-04,\n",
      "         2.9544e-03, 2.0071e-02, 2.6320e-02, 6.5649e-03, 4.6252e-03, 5.2222e-04,\n",
      "         2.6798e-03, 1.2097e-04],\n",
      "        [3.7074e-02, 7.7791e-03, 1.8153e-02, 5.3561e-03, 4.0738e-03, 1.0143e-02,\n",
      "         1.1954e-02, 1.5818e-02, 6.8312e-02, 8.7679e-03, 1.7017e-03, 9.1604e-04,\n",
      "         3.0717e-02, 3.4558e-03, 1.6302e-04, 3.0708e-02, 7.1014e-04, 7.8444e-02,\n",
      "         7.6311e-03, 1.3173e-02, 1.7753e-03, 3.1314e-02, 4.4556e-03, 3.3025e-03,\n",
      "         1.1242e-03, 5.3775e-01, 2.5718e-02, 2.3727e-02, 3.0243e-03, 2.5749e-03,\n",
      "         1.7326e-03, 8.4555e-03]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(20, 32)\n",
    "# Sample soft categorical using reparametrization trick:\n",
    "y_soft = F.gumbel_softmax(logits, tau=1, hard=False)\n",
    "print(y_soft)\n",
    "# Sample hard categorical using \"Straight-through\" trick:\n",
    "y_hard = F.gumbel_softmax(logits, tau=1, hard=True)\n",
    "print(y_hard)\n",
    "y_hard - y_soft.detach() + y_soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9000, 0.2000, 0.1000]], requires_grad=True)"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor([[0.9,0.2,0.1]], requires_grad=True)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2713, -0.3969,  0.0938],\n",
       "        [-0.1533, -0.4739, -0.1860],\n",
       "        [ 0.4577,  0.4374,  0.1955]], requires_grad=True)"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = \n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam([x1, x2],\n",
    "                    lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0450, -0.2233,  0.0075]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = torch.mm(F.gumbel_softmax(x1, dim=1), x2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0999], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = 1 - F.cosine_similarity(torch.tensor([[0.1,0.2,0.3]]), res) + F.cosine_similarity(torch.tensor([[-0.1,-0.2,-0.3]]), res)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1138,  0.6112, -0.4975]])"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2704, -0.3961,  0.0949],\n",
       "        [-0.1524, -0.4736, -0.1849],\n",
       "        [ 0.4586,  0.4381,  0.1965]], requires_grad=True)"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8997, 0.1990, 0.1009]], requires_grad=True)"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
